{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will import from DataandTokenizer, we have everything setup as if it was a non jupyter notebook\n",
    "#explanations can be found of Basic Manipulation with explanation\n",
    "\n",
    "%run DataandTokenizer.ipynb\n",
    "\n",
    "#Now that we have it run we can go ahead and start preparing our transformer \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of class Embed\n",
    "Embedding has Embedding.tokenize(data) ->(tokenizedspanish(1 per word),tokenizedpt)as dtype= Tensor\n",
    "\n",
    "\n",
    "Embedding has Embedding.detokenize(data) ->(detokenizedspanish,detokenizedpt) as dtype= Tensor\n",
    "\n",
    "Data has to have dimensions (2(spanish,portuguese),datadimension) --> [(esdata),(ptdata)]\n",
    "\n",
    "\n",
    "Be careful with detokenized as it should be decoded again!\n",
    "For now they are only crated for the specific case of spanish and pt.\n",
    "\n",
    "It will include de START AND END ALSO in both \n",
    "\n",
    "## Usage of class Data\n",
    "Data() -> Data.pt ->(Data pt) dimension(words)\n",
    "\n",
    "\n",
    "       -> Data.es ->(Data es) dimension(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testin\n",
    "\n",
    "Datos=Data()\n",
    "Embed=Embedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` no dicen , `` '' quiero mejor agua caliente en las duchas . ''\n",
      "[<tf.Tensor: shape=(2, 23), dtype=int64, numpy=\n",
      "array([[   2,   38,   38,  197,  776,   14,   38,   38,    9,    9,  324,\n",
      "         333,  442, 2805,  191,  200,   42, 1070, 2101,   16,    9,    9,\n",
      "           3],\n",
      "       [   2,  282,   16,   10,  281,   11,    3,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0]], dtype=int64)>, <tf.Tensor: shape=(2, 24), dtype=int64, numpy=\n",
      "array([[   2,   39,   39,  162,  117,  658,   28,   39,   39,    9,    9,\n",
      "         264,  373, 1950,  133,   42,  523,  201,  557, 1674,   16,    9,\n",
      "           9,    3],\n",
      "       [   2,  282,   16,   10,  209,   11,    3,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0]], dtype=int64)>]\n",
      "[<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b\"[START] ` ` no dicen , ` ` ' ' quiero mejor agua caliente en las duchas . ' ' [END]\",\n",
      "       b'[START] gracias . ( aplausos ) [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(2,), dtype=string, numpy=\n",
      "array([b\"[START] ` ` eles n\\xc3\\xa3o dizem : ` ` ' ' quero \\xc3\\xa1gua quente nos chuveiros . ' ' [END]\",\n",
      "       b'[START] obrigado . ( aplausos ) [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'],\n",
      "      dtype=object)>]\n"
     ]
    }
   ],
   "source": [
    "#TO UNDERSTAND DIMENSIONALITY\n",
    "print(Datos.es[12])\n",
    "test=[]\n",
    "testes=[]\n",
    "testpt=[]\n",
    "testes.append(Datos.es[12])\n",
    "testpt.append(Datos.pt[12])\n",
    "\n",
    "testes.append(Datos.es[13])\n",
    "testpt.append(Datos.pt[13])\n",
    "test.append(testes)\n",
    "test.append(testpt)\n",
    "print(Embed.tokenize(test))\n",
    "print(Embed.detokenize(Embed.tokenize(test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now following the structure of the transformer we need a positional codificator. We will use the one described in https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can precompute all the positional arguments in a single numpy array \n",
    "\n",
    "def angle(pos,i,dimension):\n",
    "    return pos / np.power(10000, (2 * (i//2)) / np.float32(dimension))\n",
    "   \n",
    "def positional(pos,dimension):\n",
    "    posmatrix=np.arange(pos)[:,np.newaxis]\n",
    "    dimmatrix=np.arange(dimension)[np.newaxis,:]\n",
    "    angles=angle(posmatrix,dimmatrix,dimension)\n",
    "    i=0\n",
    "    for index1,substack in enumerate(angles):\n",
    "        i=0\n",
    "        for index2,value in enumerate(substack):\n",
    "            if i%2==0:\n",
    "                substack[index2]=np.sin(value)#~we apply what is desired\n",
    "            else:\n",
    "                substack[index2]=np.cos(value)\n",
    "            i+=1\n",
    "    \n",
    "    angles=angles[np.newaxis,...]\n",
    "    #this will return a [ [](dimensions) ,... []] for every position we have dimension values think it as an embedding dimension\n",
    "    return tf.cast(angles,dtype=tf.float32)\n",
    "#And we have or positional embedding fully defined\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need padding mask and another mask to not let it read ahead.\n",
    "The padding masked just makes 0 if its non padding, meaning its non zero value, and 1 if its 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paddingmask(data):\n",
    "    #data will have dims (batchsize, sequence)\n",
    "    #for attention layers we will include two extra dimensions(batchsize,1,1,sequence)\n",
    "    data = tf.cast(tf.math.equal(data, 0), tf.float32)\n",
    "    #we do math equal in case of true cast will make it a 1.0 otherwise it will be false and cast into 0.0\n",
    "    \n",
    "    return data[:,tf.newaxis,tf.newaxis,:]\n",
    "\n",
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len\n",
    "\n",
    "def lookahead(dimension): #Dimensions has shape (dim,dim)\n",
    "    #what we want is something that has first 0.0 and 1.0 for all values in the first row and then add another 0.0 for each row until \n",
    "    #completed\n",
    "    #we will give dimension of the maximum sequence length ie(1,5) a sequence of five words \n",
    "    #in general we will have a (sequence,sequence) output dimension\n",
    "    res=np.ones(dimension)\n",
    "    for index,value in enumerate(res):\n",
    "        res[index:,index]=0.0\n",
    "        #we set from (x,x) to below as 0\n",
    "        \n",
    "    return tf.convert_to_tensor(res)\n",
    "\n",
    "\n",
    "\n",
    "#This one is the one created by tensorflow post, it will come in handy the concept is exactly the same    \n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put 0, where one would find reasonable to put 1. This is because we multiply by -inf to create a similar result when applying softmax. That is a 0 is going to have much more impact\n",
    "\n",
    "Now we are ready to tackle the attention part of the paper.\n",
    "\n",
    "\n",
    "# Dot product attention\n",
    "The structure and formulas can be found on the paper. We need some set of Queries, Keys and Values that encapsulate the information between relationship of words and the value of a set word in a sentence.\n",
    "\n",
    "\n",
    "A relly important step is that we sum the mask to the result that we are achieving multiplied to a value close to -inf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotproductattention(q,k,v,mask):\n",
    "    \n",
    "    \n",
    "    #First we need to matmul q,k transposing k\n",
    "    resmatmul=tf.matmul(q,k,transpose_b=True)\n",
    "    \n",
    "    #then we need the dimension of k d_k \n",
    "    kdim=float(tf.shape(k)[-1])\n",
    "    resmatmul=resmatmul/(tf.math.sqrt(kdim))\n",
    "    \n",
    "    #It may be useful to include the situation where mask is None\n",
    "    if mask is not None:\n",
    "        resmatmul += mask*-1e9\n",
    "    \n",
    "    ressoftmax=tf.nn.softmax(resmatmul,axis=-1) # we need to do softmax on the last dimension\n",
    "    return tf.matmul(ressoftmax,v), ressoftmax\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_k = tf.constant([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[10, 0, 0]], dtype=tf.float32)  # (1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[1.000000e+00, 9.276602e-25]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
       " array([[1.0000000e+00, 8.4332744e-26, 8.4332744e-26, 8.4332744e-26]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dotproductattention(temp_q,temp_k,temp_v,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains now is to create the multi head attention. The powerful idea of multi-head is the usage of parallel computation so what the papers does is project in a number of dotproductattention layers, and then concatanate the result\n",
    "\n",
    "\n",
    "Also each queries,keys and values will need their respective weights as described in the paper. That is a keras Dense model(lineal). We will call the dimension of these weights the modeldim. \n",
    "\n",
    "\n",
    "\n",
    "We need to create a class that is a Layer of keras that is\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self,modeldim,layersnum): \n",
    "        # we need to inherit all of the keras information of th super\n",
    "        super().__init__()\n",
    "        #now we can start working\n",
    "        \n",
    "        #WEIGHTS FOR Q,K,V\n",
    "        self.wq=tf.keras.layers.Dense(modeldim)\n",
    "        self.wk=tf.keras.layers.Dense(modeldim)\n",
    "        self.wv=tf.keras.layers.Dense(modeldim)\n",
    "                          \n",
    "            \n",
    "        #WEIGHTS FO AFTER CONCATENTION\n",
    "        self.dense=tf.keras.layers.Dense(modeldim)\n",
    "        \n",
    "        #extra information\n",
    "        \n",
    "        self.modeldim=modeldim\n",
    "        self.layersnum=layersnum\n",
    "        assert modeldim % self.layersnum == 0\n",
    "        self.depth=modeldim//self.layersnum\n",
    "        \n",
    "        \n",
    "    #WE NEED TO SPLIT INTO modeldim /layresnum\n",
    "    \n",
    "    #All this part is heavily optimized thanks to the tensorflow tutorial for transformers\n",
    "    #The main idea is to split the last dimension into (number of layers, model dimension)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        #Split the last dimension into (layersnum, depth).\n",
    "        #Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        #\n",
    "        x = tf.reshape(x, (batch_size, -1, self.layersnum, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, layersnum, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, layersnum, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, layersnum, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, layersnum, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, layersnum, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = dotproductattention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.modeldim))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need what in the paper is described as Position-wise Feed-Forward Networks. Its simply a dense layer, with a relu applied first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "          tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER AND DECODER\n",
    "\n",
    "Now we have mostly developed most of the important stuff, what remains is to create the encoders as described by the paper. For simplicity we will use the implimentation used in the tensorflow post about transformers, these classes can be found online.\n",
    "\n",
    "Nothing is really complicated, just following the structure described in the paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#here we have the definition of the layers\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        #we can see that q,k,v is the same vector given for encoders\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model) we do an attention\n",
    "        attn_output = self.dropout1(attn_output, training=training)#droupout for better training\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model) normalization\n",
    " \n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model) feedforwards\n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model) \n",
    "\n",
    "        return out2\n",
    "    \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#AND finally we define the encoder and the decoder\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional(maximum_position_encoding,\n",
    "                                                self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model) we create the embedding so it is of the dimensions we want\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        print(x)\n",
    "        print(self.pos_encoding)\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate)\n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,\n",
    "               look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                                                 look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "We finally can create the transformer. In reality, only remains to put the pieces back together. We will again use the tensorflow implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "                   target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff,\n",
    "                                 input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,\n",
    "                               target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "    # Keras models prefer if you pass all your inputs in the first argument\n",
    "        inp, tar = inputs\n",
    "\n",
    "        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n",
    "\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n",
    "\n",
    "    def create_masks(self, inp, tar):\n",
    "        # Encoder padding mask\n",
    "        enc_padding_mask = paddingmask(inp)\n",
    "\n",
    "        # Used in the 2nd attention block in the decoder.\n",
    "        # This padding mask is used to mask the encoder outputs.\n",
    "        dec_padding_mask = paddingmask(inp)\n",
    "\n",
    "        # Used in the 1st attention block in the decoder.\n",
    "        # It is used to pad and mask future tokens in the input received by\n",
    "        # the decoder.\n",
    "        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "        dec_target_padding_mask = create_padding_mask(tar) #lookaheadmask\n",
    "        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "        return enc_padding_mask, look_ahead_mask, dec_padding_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
