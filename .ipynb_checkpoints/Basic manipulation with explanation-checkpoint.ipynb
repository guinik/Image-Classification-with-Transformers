{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic importations\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text as text\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is mainly for deep understanding of paper Attention is all you need \n",
    "## Check it out here : https://arxiv.org/abs/1706.03762 .  \n",
    "Basically is creating a transformer to be able to translate words from english to portuguese in this specific case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the data provided by Ted fundation in tensorflow_datasets\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/es_to_pt', with_info=True,\n",
    "                               as_supervised=True)\n",
    "\n",
    "\n",
    "\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will call data a class to not have a random global variable \n",
    "class Data():\n",
    "    def __init__(self):\n",
    "        #alternitavely we can ask in innit what language we want to train\n",
    "        i=0\n",
    "        #WE WILL GET THE DATASET IN WORKING CONDITIONS EVERY LIST(TRAIN_EXMAPLES[I]) GIVES US A (EXAMPLE 1, EXAMPLE 2)\n",
    "        pt=[]\n",
    "        es=[]\n",
    "        #WE NEED TO DECODE IT TO UTF-8 SO IT IS PRESENTABLE AND WE ONLY WANT THE NUMPY\n",
    "        for es_example, pt_example in list(train_examples):\n",
    "            pt.append((pt_example.numpy()).decode('utf-8'))\n",
    "            es.append((es_example.numpy()).decode('utf-8'))\n",
    "        self.pt=pt\n",
    "        self.es=es\n",
    "        #And with this we have or data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to think about input embedding .\n",
    "Because obviously computers dont understand strings, we need some kind of input. We will build an embedding from zero\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset -> will help us generate a word for word construction\n",
    "But for this case we need it to be object database so Data() is not useful in this case, although we will use it later\n",
    "\n",
    "\n",
    "BertTokenizer is a type of embedding described in detail in https://arxiv.org/abs/1810.04805, we will not go in detail, we will assume its effectivness in embedding (as they describe is a bidirectional representation from unlabeled text )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2052\\1744158513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# WE USE MAP AND LAMBDA MAP MAPS ONTO ARRAY AND SIMPLY LAMBDA ES,PT: ES IS SETTING FROM BOTH ES AND PT SETTING THE RESULT TO ES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# THE RESEVED TOKENS ARE SIMPLY START FINISH conditions (recall we need to start and finish the input and output)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mes_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_vocab_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreserved_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"[PAD]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[UNK]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[START]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[END]\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mpt_vocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert_vocab_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_examples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreserved_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"[PAD]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[UNK]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[START]\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"[END]\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow_text\\tools\\wordpiece_vocab\\bert_vocab_from_dataset.py\u001b[0m in \u001b[0;36mbert_vocab_from_dataset\u001b[1;34m(dataset, vocab_size, reserved_tokens, bert_tokenizer_params, learn_params)\u001b[0m\n\u001b[0;32m     89\u001b[0m   \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbert_tokenizer_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m   \u001b[0mwords_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m   \u001b[0mword_counts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m   vocab = learner.learn(word_counts, vocab_size, reserved_tokens,\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow_text\\tools\\wordpiece_vocab\\wordpiece_tokenizer_learner_lib.py\u001b[0m in \u001b[0;36mcount_words\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    389\u001b[0m   \u001b[1;34m\"\"\"Converts a iterable of arrays of words into a `Counter` of word counts.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m   \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 391\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m     \u001b[1;31m# Convert a RaggedTensor to a flat/dense Tensor.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'flat_values'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 766\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    767\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    768\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    754\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[1;31m# Fast path for the case `self._structure` is not a nested structure.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py\u001b[0m in \u001b[0;36m_from_compatible_tensor_list\u001b[1;34m(self, tensor_list)\u001b[0m\n\u001b[0;32m   2585\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_values_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2586\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Customized value_type is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2587\u001b[1;33m     result = RaggedTensor._from_variant(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2588\u001b[0m         \u001b[0mtensor_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2589\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py\u001b[0m in \u001b[0;36m_from_variant\u001b[1;34m(cls, variant, dtype, output_ragged_rank, input_ragged_rank, row_splits_dtype, name)\u001b[0m\n\u001b[0;32m   2014\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RaggedFromVariant\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2015\u001b[0m         [variant, dtype, input_ragged_rank, output_ragged_rank]):\n\u001b[1;32m-> 2016\u001b[1;33m       result = gen_ragged_conversion_ops.ragged_tensor_from_variant(\n\u001b[0m\u001b[0;32m   2017\u001b[0m           \u001b[0mvariant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ragged_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_ragged_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m           row_splits_dtype, name)\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\tensorflow\\python\\ops\\gen_ragged_conversion_ops.py\u001b[0m in \u001b[0;36mragged_tensor_from_variant\u001b[1;34m(encoded_ragged, input_ragged_rank, output_ragged_rank, Tvalues, Tsplits, name)\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RaggedTensorFromVariant\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoded_ragged\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;34m\"input_ragged_rank\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_ragged_rank\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_ragged_rank\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Datos=Data()\n",
    "# in this case lets generate a vocabulary of 5000 words\n",
    "# WE USE MAP AND LAMBDA MAP MAPS ONTO ARRAY AND SIMPLY LAMBDA ES,PT: ES IS SETTING FROM BOTH ES AND PT SETTING THE RESULT TO ES\n",
    "# THE RESEVED TOKENS ARE SIMPLY START FINISH conditions (recall we need to start and finish the input and output)\n",
    "es_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:es),vocab_size =5000,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n",
    "pt_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:pt),vocab_size =5000,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_vocab_file(filepath, vocab):\n",
    "    with open(filepath, 'w',encoding='utf-8') as f:\n",
    "        for token in vocab:\n",
    "              print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file('es_vocab.txt',es_vocab)\n",
    "write_vocab_file('pt_vocab.txt',pt_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this we have already created a vocabulary. To  encapsulate everything we could put it in a class in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabgen:\n",
    "    def __init__(self,filename1,filename2,num):\n",
    "        es_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:es),vocab_size =num,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n",
    "        pt_vocab = bert.bert_vocab_from_dataset(train_examples.map(lambda es,pt:pt),vocab_size =num,reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"])\n",
    "        #WE CREATE THE VOCABS AND WE WRITE THEM\n",
    "        write_vocab_file(filename1,es_vocab)\n",
    "        write_vocab_file(filename2,pt_vocab)\n",
    "        #We could make it even better so that given two languages we create the vocabs but is not necessary at the moment\n",
    "        \n",
    "    def write_vocab_file(filepath, vocab):\n",
    "        with open(filepath, 'w',encoding='utf-8') as f:\n",
    "            for token in vocab:\n",
    "                  print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Vocabgen at 0x1c56b50d550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vocabgen('es_vocab.txt','pt_vocab.txt',5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily create the tokenizers using tensorflow_text.BertTokenizer(atext, **kargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        #the tokenizer class will have tokens of class BertTokenizer as .pr .es \n",
    "        self.pt = text.BertTokenizer('pt_vocab.txt', **{})\n",
    "        self.es =text.BertTokenizer('es_vocab.txt', **{})\n",
    "\n",
    "def add_start_end(ragged): #this function has been copied from tensorflow  guide to subwords\n",
    "        #will add the start and end\n",
    "        \n",
    "        reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "        START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n",
    "        END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n",
    "        count = ragged.bounding_shape()[0]\n",
    "        starts = tf.fill([count,1], START)\n",
    "        ends = tf.fill([count,1], END)\n",
    "        return tf.concat([starts, ragged, ends], axis=1)\n",
    "        \n",
    "#AND WE CREATE ANOTHER CLASS THAT CONTAINS OUR TOKENIZER TO TOKENIZE AND DE-TOKENIZE\n",
    "class Embedding:\n",
    "    Tokens=Tokenizer()\n",
    "        #tokenize ---->\n",
    "        #This returns an output of (batch,word,wordpiece)\n",
    "        #What we mean is that we have -> I am good. output-> [ [ [I(values)] , [am(values)] , [good(values)]] ]\n",
    "    \n",
    "    def detokenize(self,detokenizedata):\n",
    "        detokenizept=(Tokens.pt.detokenize(detokenizedata[1]))\n",
    "        detokenizees=(Tokens.es.detokenize(detokenizedata[0]))\n",
    "        \n",
    "        detokenizees= tf.strings.reduce_join(detokenizees, separator=' ', axis=-1)\n",
    "        detokenizept= tf.strings.reduce_join(detokenizept, separator=' ', axis=-1)\n",
    "        merged=[]\n",
    "        merged.append(detokenizees)\n",
    "        merged.append(detokenizept)\n",
    "        return merged\n",
    "    def tokenize(self,tokenizedata):\n",
    "        tokenizept=(Tokens.pt.tokenize(tokenizedata[1])).merge_dims(1,2)\n",
    "        tokenizees=(Tokens.es.tokenize(tokenizedata[0])).merge_dims(1,2)\n",
    "        #this part is for preperation for our purposes\n",
    "        tokenizept=(add_start_end(tokenizept)).to_tensor()# we apply to tensor to make the output a normal tensor\n",
    "        tokenizees=(add_start_end(tokenizees)).to_tensor()# we apply to tensor to make the output a normal tensor\n",
    "        \n",
    "        #THIS TO TENSOR WILL MAKE THAT ON TH OUTPUT WE GET A LOT OF PADDING but thats easily removable\n",
    "        \n",
    "        merged=[]\n",
    "        merged.append(tokenizees)\n",
    "        merged.append(tokenizept)\n",
    "        return merged\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datos=Data()\n",
    "Embed=Embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the conversions lets do a simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`` eles não dizem : `` '' quero água quente nos chuveiros . ''\n",
      "[<tf.Tensor: shape=(2, 23), dtype=int64, numpy=\n",
      "array([[   2,  282,   16,   10,  281,   11,    3,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0],\n",
      "       [   2,   38,   38,  197,  776,   14,   38,   38,    9,    9,  324,\n",
      "         333,  442, 2805,  191,  200,   42, 1070, 2101,   16,    9,    9,\n",
      "           3]], dtype=int64)>, <tf.Tensor: shape=(2, 24), dtype=int64, numpy=\n",
      "array([[   2,   39,   39,  162,  117,  658,   28,   39,   39,    9,    9,\n",
      "         264,  373, 1950,  133,   42,  523,  201,  557, 1674,   16,    9,\n",
      "           9,    3],\n",
      "       [   2,  282,   16,   10,  209,   11,    3,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0]], dtype=int64)>]\n",
      "[START] obrigado . ( aplausos ) [END] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "obrigado . ( aplausos )\n"
     ]
    }
   ],
   "source": [
    "test=[]\n",
    "testes=[]\n",
    "testpt=[]\n",
    "testpt.append(Datos.pt[12])#WE INCLUDE TWO EXAMPLES SO WE CHECK VARIABILITY OF BATCH SIZE\n",
    "testpt.append(Datos.pt[13])\n",
    "testes.append(Datos.es[13])\n",
    "testes.append(Datos.es[12])\n",
    "print(Datos.pt[12])\n",
    "test.append(testes)\n",
    "test.append(testpt)\n",
    "tokenizeddata=Embed.tokenize(test)\n",
    "#We will try to detokenize to see if its working!\n",
    "print(tokenizeddata)\n",
    "\n",
    "print((Embed.detokenize(tokenizeddata)[1].numpy())[1].decode('utf-8')) #this goes to the pt detokenize data and decodes \n",
    "#it in a tensor\n",
    "#then we are specifically getting the first text and decoding it  \n",
    "print((Datos.pt[13]))\n",
    "#ITS REALLY IMPORTAnt to decode de output, it will be coded in utf-8 to make it leggible we need to decode \n",
    "#AND as it can be seen we have a desired output only some random spaces that we do not need to bother about as differences\n",
    "#IT IS ALSO INCLUDED THE START AND END WHICH IS EXACTLY WHAT WE NEED\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of class Embed\n",
    "Embed has Embed.tokenizedata(data) ->(tokenizedspanish,tokenizedpt)as dtype= Tensor\n",
    "\n",
    "\n",
    "Embed has Embed.detokenizedata(data) ->(detokenizedspanish,detokenizedpt) as dtype= Tensor\n",
    "\n",
    "\n",
    "Be careful with detokenized as it should be decoded again!\n",
    "For now they are only crated for the specific case of spanish and pt.\n",
    "\n",
    "It will include de START AND END ALSO in both \n",
    "\n",
    "## Usage of class Data\n",
    "Data() -> Data.pt ->(Data pt)\n",
    "\n",
    "\n",
    "       -> Data.es ->(Data es) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL IS HEAVILY INFLUENCED BY https://www.tensorflow.org/text/guide/subwords_tokenizer?hl=es-419%C3%A7\n",
    "\n",
    "As it is just a learning project"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
